{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在PyTorch中，自定义数据类通常需要继承`torch.utils.data.Dataset`类，并实现`__init__`、`__len__`和`__getitem__`方法。以下是一个详细的示例，它展示了如何创建一个自定义数据集类。\n",
    "\n",
    "## 自定义数据集类示例\n",
    "\n",
    "假设有一个自定义的数据集，其中包含图片和对应的标签。可以按照以下步骤构建一个自定义数据集类：\n",
    "\n",
    "```python\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): 包含图像路径和标签的信息的CSV文件路径。\n",
    "            root_dir (string): 图像文件夹的路径。\n",
    "            transform (callable, optional): 可选的转换，应用于样本。\n",
    "        \"\"\"\n",
    "        self.labels_df = pd.read_csv(csv_file)  # 读取标签\n",
    "        self.root_dir = root_dir  # 图像目录\n",
    "        self.transform = transform  # 转换\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"返回数据集的样本数量\"\"\"\n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"根据索引获取一条数据（图像和标签）\"\"\"\n",
    "        img_name = os.path.join(self.root_dir, self.labels_df.iloc[idx, 0])  # 获取图像路径\n",
    "        image = Image.open(img_name)  # 打开图像\n",
    "        label = self.labels_df.iloc[idx, 1]  # 获取对应标签\n",
    "\n",
    "        # 应用转换\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "```\n",
    "\n",
    "### 使用自定义数据集类\n",
    "\n",
    "一旦定义了数据集类，就可以使用它来创建数据加载器。以下是如何使用刚才定义的 `CustomImageDataset` 类的示例：\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 1. 定义图像转换\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),  # 将图像调整为28x28\n",
    "    transforms.ToTensor(),  # 转换为Tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # 归一化\n",
    "])\n",
    "\n",
    "# 2. 创建数据集实例\n",
    "data_path = 'dataset/labels.csv'  # CSV文件路径\n",
    "image_folder = 'dataset/images/'  # 图像目录\n",
    "dataset = CustomImageDataset(csv_file=data_path, root_dir=image_folder, transform=transform)\n",
    "\n",
    "# 3. 创建数据加载器\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "\n",
    "# 4. 测试数据加载器\n",
    "for images, labels in train_loader:\n",
    "    print(images.shape)  # 打印图像的形状\n",
    "    print(labels)  # 打印标签\n",
    "    break  # 只展示第一批数据\n",
    "```\n",
    "\n",
    "### 解释\n",
    "\n",
    "1. **`__init__` 方法**：读取 CSV 文件中的标签，并保存图像文件的根目录和可选的转换。\n",
    "2. **`__len__` 方法**：返回数据集中样本的数量。\n",
    "3. **`__getitem__` 方法**：根据给定的索引加载图像和标签，在加载图像后应用转换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 自定义数据集类\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images  # 图像特征\n",
    "        self.labels = labels  # 标签\n",
    "        self.transform = transform  # 转换\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)  # 返回样本数量\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]  # 获取图像路径\n",
    "        image = Image.open(img_name)  # 打开图像\n",
    "        label = self.labels[idx]  # 获取标签\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)  # 应用转换\n",
    "\n",
    "        return image, label  # 返回图像和标签\n",
    "\n",
    "# 假设我们有数据框 df，包含图像路径和标签\n",
    "# df = pd.read_csv('dataset/labels.csv')  # 示例\n",
    "\n",
    "# 获取图像路径和标签\n",
    "df = pd.read_csv('path')\n",
    "\n",
    "images = df['filename'].values\n",
    "labels = df['label'].values\n",
    "\n",
    "# 使用 train_test_split 拆分数据集\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# 进一步拆分训练集为训练集和验证集\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)  # 新的训练集占60%\n",
    "\n",
    "# 定义转换\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# 创建数据集实例\n",
    "train_dataset = CustomImageDataset(X_train, y_train, transform=transform)\n",
    "val_dataset = CustomImageDataset(X_val, y_val, transform=transform)\n",
    "test_dataset = CustomImageDataset(X_test, y_test, transform=transform)\n",
    "\n",
    "# 创建数据加载器\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "from torchvision import transforms as TF\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from transformers import SegformerForSemanticSegmentation\n",
    "from transformers import get_scheduler\n",
    "\n",
    "from sklearn.metrics import jaccard_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BDDDataset(Dataset):\n",
    "    def __init__(self, images_dir, masks_dir, transform=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.transform = transform\n",
    "        self.images = [img for img in os.listdir(images_dir) if img.endswith('.jpg')]\n",
    "        self.masks = [mask.replace('.jpg', '.png') for mask in self.images]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.images_dir, self.images[idx])\n",
    "        mask_path = os.path.join(self.masks_dir, self.masks[idx])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert('L')  # Convert mask to grayscale.\n",
    "        \n",
    "        # Convert mask to binary format with 0 and 1 values.\n",
    "        mask = np.array(mask)\n",
    "        mask = (mask > 0).astype(np.uint8)  # Assuming non-zero pixels are lanes.\n",
    "        \n",
    "        # Convert to PIL Image for consistency in transforms.\n",
    "        mask = Image.fromarray(mask)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            # Assuming to_tensor transform is included which scales pixel values between 0-1.\n",
    "            # mask = to_tensor(mask)  # Convert the mask to [0, 1] range.\n",
    "        mask = TF.functional.resize(img=mask, size=[360, 640], interpolation=Image.NEAREST)\n",
    "        mask = TF.functional.to_tensor(mask)\n",
    "        mask = (mask > 0).long()  # Threshold back to binary and convert to LongTensor.\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "def mean_iou(preds, labels, num_classes):\n",
    "    # Flatten predictions and labels.\n",
    "    preds_flat = preds.view(-1)\n",
    "    labels_flat = labels.view(-1)\n",
    "\n",
    "    # Check that the number of elements in the flattened predictions.\n",
    "    # and labels are equal.\n",
    "    if preds_flat.shape[0] != labels_flat.shape[0]:\n",
    "        raise ValueError(f\"Predictions and labels have mismatched shapes: \"\n",
    "                         f\"{preds_flat.shape} vs {labels_flat.shape}\")\n",
    "\n",
    "    # Calculate the Jaccard score for each class.\n",
    "    iou = jaccard_score(labels_flat.cpu().numpy(), preds_flat.cpu().numpy(),\n",
    "                        average=None, labels=range(num_classes))\n",
    "\n",
    "    # Return the mean IoU.\n",
    "    return np.mean(iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the appropriate transformations.\n",
    "transform = TF.Compose([\n",
    "    TF.Resize((360, 640)),\n",
    "    TF.ToTensor(),\n",
    "    TF.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create the dataset.\n",
    "train_dataset = BDDDataset(images_dir='./deep_drive_10K/train/images',\n",
    "                           masks_dir='./deep_drive_10K/train/masks',\n",
    "                           transform=transform)\n",
    "\n",
    "valid_dataset = BDDDataset(images_dir='./deep_drive_10K/valid/images',\n",
    "                           masks_dir='./deep_drive_10K/valid/masks',\n",
    "                           transform=transform)\n",
    "\n",
    "# Create the data loaders.\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=6)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=4, shuffle=False, num_workers=6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xy_ultra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
